---
title: "Comparison Numerical Optimization Methods"
author: "Sandra Rudeloff"
date: "2023-08-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Derivation of the likelihood function

Likelihood function:
$$\mathcal{L}( \theta, \rho) = - \sum_{i=1}^p \mu_i + \sum_{i=1}^p y_i \log{\mu_i}$$

-   $\theta$: risk parameters, in my case $\alpha$ and $\beta$
-   $p$: number of subregions - in my case 100
-   $y_i$: number of cases in each subregion
-   $\mu_i$ expected number of cases in subregion $i$

Expected Number of cases: $$\mu_i = \rho N_i \prod_{l=1}^{k} f_l(x_i)$$

-   $\rho$: scale parameter
-   $N_i$: number of people at risk in subregion $i$, in my case the
    population
-   $f_l(x_i)$: risk function associated with the $l^{th}$ source,
    evaluated at the centroid $x_i$ of subregion $i$.

risk function for distance $d$ from the source:
$$f(d) = 1 + \alpha \exp[- (\frac{d}{\beta})^2]$$

-   $\alpha$: proportional elevation in risk at the source
-   $\beta$: decrease in risk

Given this information, you can use the likelihood function to estimate
the parameters $\alpha, \beta$ and $\rho$ for the model without
covariates.

# Libraries

```{r}
library(readxl)# Read Scenario Definition from Excel
library(dplyr)

# Visualization
library(ggplot2)
library(ggnewscale)
library(RColorBrewer)
library(plotly)

library(reticulate) # connection to python scripts
library(DEoptim) # optimizer

source("generate_population.R")
source("generate_supermarkets.R")
source("visualize_scenario.R")
source_python("../Diffusion Model/gravity_model.py")
source_python("../Diffusion Model/outbreak_generation.py")
```

# Input Data
```{r}
investigation_scenario <- 1
no_of_cells <- 100
delta <- 0.05  # Define half the side length of a square
set.seed(333)
```

# Collect Variables
All values are measured in km.

## Population Data

number of people at risk in each subregion $N$
```{r}
population_data <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Population"),scenario_id == investigation_scenario) 
```

```{r}
df_population <- generate_population(population_data, no_of_cells) 
```

```{r}
df_population$cell_id <- as.numeric(row.names(df_population))
```


## Shops Data
```{r}
chain_details <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Chain_Details"), scenario_id == investigation_scenario)
```

```{r}
df_shops <- data.frame()

# Loop through each chain in the specific scenario to create the final dataframe
for (current_chain_id in unique(chain_details$chain_id)) {
  chain_data <- chain_details %>% 
    filter(chain_id == current_chain_id)
  
  df_shops_current <- generate_shops(no_of_cells, chain_data)
  df_shops_current$chain <- current_chain_id
  
  df_shops <- rbind(df_shops, df_shops_current)
}
```

```{r}
# Assign the correct cell_id to the stores
df_shops <- df_shops %>%
  rowwise() %>%
  mutate(
    cell_id = which(
      (df_population$x_centroid - 0.05) <= store_x &
      (df_population$x_centroid + 0.05) >= store_x &
      (df_population$y_centroid - 0.05) <= store_y &
      (df_population$y_centroid + 0.05) >= store_y
    )
  ) %>%
  ungroup()
```

## Outbreak Data
Convert the df_shops data frame to a Python data frame
```{r}
df_shops_py <- r_to_py(df_shops)
df_population_py <- r_to_py(df_population)
```

```{r}
outbreak_data <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Outbreak"), scenario_id == investigation_scenario)
```

### Calculate Flow of Goods
```{r}
empirical_mean_shopping_distance <- outbreak_data$empirical_mean_shopping_distance
tolerance <- outbreak_data$tolerance
```

```{r}
flow <- hyman_model(empirical_mean_shopping_distance, tolerance,df_population_py, df_shops_py)
```

### Generate Outbreaks for Scenario
```{r}
if (is.character(outbreak_data$outbreak_scenario_sizes)) {
  list_outbreak_scenario_sizes <- as.integer( unlist(strsplit(outbreak_data$outbreak_scenario_sizes, ",")))
} else {
  list_outbreak_scenario_sizes <- as.integer(outbreak_data$outbreak_scenario_sizes)
}

no_of_trials_per_scenario = as.integer(outbreak_data$no_of_trials_per_scenario) #Hier könnte man auch noch eine Logik hinterlegen
```


```{r}
unique_chains <- unique(df_shops$chain)
```

```{r}
# Create a list to store the outbreak data
outbreak_list <- list()

# Loop over each chain, outbreak scenario size, and trial number
for (chain in unique_chains) {
  for (no_of_outbreak_cases in list_outbreak_scenario_sizes) {
    for (trial in seq_len(no_of_trials_per_scenario)) {
      
      # Generate the outbreak data
      outbreak <- generate_outbreak(chain, no_of_outbreak_cases, flow, df_shops_py, df_population_py)
      
      # Create a unique identifier for this outbreak scenario
      identifier <- paste(chain, no_of_outbreak_cases, trial, sep = "_")
      
      # Save the outbreak data to the list using the identifier as the name
      outbreak_list[[identifier]] <- outbreak
    }
  }
}
```


```{r}
df_outbreak <- outbreak_list$`Chain 1_10_1`
```



## Visualization / Plotting

```{r}
visualize_scenario(investigation_scenario, df_shops, df_population, df_outbreak)
```


# Model

## Construct Y

Observed number of cases in each subregion

```{r}
# function to check if a point is within a square centered at (x, y) with side length 2*delta
point_in_square <- function(px, py, x, y, delta) {
  return(px >= (x - delta) & px <= (x + delta) & py >= (y - delta) & py <= (y + delta))
}

y <- numeric(nrow(df_population))
for (i in 1:nrow(df_population)) {
  y[i] <- sum(point_in_square(df_outbreak$case_x, df_outbreak$case_y, df_population$x_centroid[i], df_population$y_centroid[i], delta))
}
```

## Construct N

Number of people at risk in each subregion

```{r}
N <- df_population$population
```

## Helper functions

```{r}
# Calculate the Risk Function for each Source and subregion
# Compute distance between two points
compute_distance <- function(x1, y1, x2, y2) {
  return(sqrt((x1 - x2)^2 + (y1 - y2)^2))
}

# Risk function for distance d from the source
risk_function <- function(d, alpha, beta) {
  return(1 + alpha * exp(- (d/beta)^2))
}
```

## Risk matrix

The risk matrix contains on the rows the cells, 1 being in the left lower corner, and the stores on the columns.

```{r}
compute_risk_matrix <- function(df_population, df_shops, alpha, beta) {
  risk_matrix <- matrix(0, nrow(df_population), nrow(df_shops))
  for (i in 1:nrow(df_population)) {
    for (j in 1:nrow(df_shops)) {
      d <- compute_distance(df_population$x_centroid[i], df_population$y_centroid[i], df_shops$store_x[j], df_shops$store_y[j])
      risk_matrix[i, j] <- risk_function(d, alpha, beta)
    }
  }
  return(risk_matrix)
}
```

## Likelihood

### Alternative
The expected value mu is the overall prevlance of the disease (rho) times the number of population in that cell times the risk_matrix value at that position
mu enhtält einen value per cell. 

Erstmal addieren wir alle expected values und die sind dann negativ und dann 
```{r}
likelihood_function_minimize <- function(params, y, N) {
  alpha <- params[1]
  beta <- params[2]
  rho <- sum(y) / sum(df_population$population)
  #rho <- params[3]
  #print(paste("alpha: ", alpha, "beta: ", beta, "rho: ", rho))
  
  if (alpha < 0 || beta < 0) {
    return(1e6)
  }

  risk_matrix <- compute_risk_matrix(df_population, df_shops, alpha, beta)
  mu <- rho * N * apply(risk_matrix, 1, prod)

  log_likelihood <- -sum(mu) + sum(y * log(mu)) # without y! because it does not do anything for the optimization
  #print(paste("likelihood: ", log_likelihood))
  
  if (is.nan(log_likelihood)) {
    return(1e6)
  }
  return(-log_likelihood) # We return the negative likelihood because optimizers in R typically minimize
}
```

```{r}
likelihood_function_maximize <- function(params, y, N) {
  alpha <- params[1]
  beta <- params[2]
  rho <- sum(y) / sum(df_population$population)
  #rho <- params[3]
  #print(paste("alpha: ", alpha, "beta: ", beta, "rho: ", rho))
  
  if (alpha < 0 || beta < 0) {
    return(1e6)
  }

  risk_matrix <- compute_risk_matrix(df_population, df_shops, alpha, beta)
  mu <- rho * N * apply(risk_matrix, 1, prod)

  log_likelihood <- -sum(mu) + sum(y * log(mu)) # without y! because it does not do anything for the optimization
  #print(paste("likelihood: ", log_likelihood))
  
  if (is.nan(log_likelihood)) {
    return(-1e6)
  }
  return(log_likelihood)
}
```


## Optimize
The max. distance that can occur in the toy example is 
> compute_distance(0.05, 0.05, 0.95, 0.95)
[1] 1.272792

so any value for beta over 50 doesn't make any sense 
```{r}
lower_bounds <- c(alpha = 0.001, beta = 0.0001)
upper_bounds <- c(alpha = 5000, beta = 50) 
```

```{r}
logLik_null <- -likelihood_function_minimize(c(0,0), y=y, N=N)
```


### Deoptim

#### Alternative Model

```{r}
result_alternative_DEoptim <- DEoptim(fn = likelihood_function_minimize, lower = lower_bounds, upper = upper_bounds, y = y, N = N)
print(result_alternative_DEoptim$optim$bestmem)
```

#### Compare the nested Models
```{r}
logLik_alternative_DEoptim <- -result_alternative_DEoptim$optim$bestval
```

```{r}
GLRT_statistic <- 2 * (logLik_alternative_DEoptim - logLik_null) #y! kürzt sich raus

# Determine the degrees of freedom (difference in number of parameters between the two models)
df <- 2  # alpha and beta are the additional parameters in the alternative model

# Compute the p-value
p_value <- 1 - pchisq(GLRT_statistic, df)

# Print the results
print(paste("GLRT statistic:", GLRT_statistic))
# print(paste("Degrees of freedom:", df))
print(paste("P-value:", p_value))

# Decide on the hypothesis based on a significance level (e.g., 0.05)
if (p_value < 0.05) {
  cat("Reject the null hypothesis in favor of the alternative.\n")
} else {
  cat("Fail to reject the null hypothesis.\n")
}
```


# Summary
## DeOptim
```{r}
print(paste("alpha: ", result_alternative_DEoptim$optim$bestmem[1], " beta: ", result_alternative_DEoptim$optim$bestmem[2]))
print(paste("likelihood value: ", likelihood_function_minimize(c(result_alternative_DEoptim$optim$bestmem[1], result_alternative_DEoptim$optim$bestmem[2]), y=y, N=N)))
```

```{r}
likelihood_function_minimize(c(0, 0), y=y, N=N)
```

```{r}
wrapper_likelihood_function <- function(params) {
  y_fixed <- y  # Replace with the actual y values
  N_fixed <- N  # Replace with the actual N values
  return(likelihood_function_minimize(params, y_fixed, N_fixed))
}
```

# Standard error
## Hessian 
```{r}
library(numDeriv)
hessian_matrix <- hessian(wrapper_likelihood_function, c(result_alternative_DEoptim$optim$bestmem[1], result_alternative_DEoptim$optim$bestmem[2]))
inverse_hessian <- solve(hessian_matrix)
standard_errors <- sqrt(diag(inverse_hessian))
```
standard error gives an idea of how much the estimated parameter is expected to vary from sample to sample.

In your specific case, you're dealing with a likelihood function for which you've estimated parameters (alpha and beta). The Hessian matrix at the maximum likelihood estimates gives you an idea of how "curved" the likelihood function is at that point. The inverse of this Hessian matrix is related to the variances (and covariances) of your parameter estimates. Taking the square root of the diagonal elements gives you the standard errors, which tell you how much those estimates are likely to vary if you were to collect new data and re-estimate the parameters.

## Monte Carlo
mplementing Monte Carlo simulations to estimate standard errors involves generating multiple simulated datasets based on your fitted model, then refitting the model to each of these datasets to obtain a distribution of parameter estimates. The standard deviation of this distribution will give you the standard error for each parameter.
```{r}
# Initialize variables to store parameter estimates
alpha_estimates <- numeric()
beta_estimates <- numeric()

# Number of simulations
n_simulations <- 1000

# Loop over simulations
for (i in 1:n_simulations) {
  
  risk_matrix <- compute_risk_matrix(df_population, df_shops, result_alternative_DEoptim$optim$bestmem[1], result_alternative_DEoptim$optim$bestmem[2])
  rho <- sum(y) / sum(df_population$population)
  
  mu <- rho * N * apply(risk_matrix, 1, prod)
  
  # Step 2: Generate simulated data (this will depend on your specific model)
  simulated_y <- rpois(length(mu), lambda = mu)
  
  # Step 3: Fit the model to the simulated data
  result_simulated <- DEoptim(fn = likelihood_function_minimize, lower = lower_bounds, upper = upper_bounds, y = simulated_y, N = N)#,  DEoptim.control(itermax = 100))
  
  # Store the estimated parameters
  alpha_estimates[i] <- result_simulated$optim$bestmem[1]
  beta_estimates[i] <- result_simulated$optim$bestmem[2]
  
}

# Step 4: Calculate standard errors
se_alpha <- sd(alpha_estimates)
se_beta <- sd(beta_estimates)

# Print standard errors
print(paste("Standard error of alpha: ", se_alpha))
print(paste("Standard error of beta: ", se_beta))

```



# Confidence Intervals
Calculate Confidence Intervals: Use the formula for a confidence interval, which is:

Confidence Interval = Parameter Estimate ± (Z-value × Standard Error)

```{r}
z_value <- 1.96 # For a 95% confidence level
lower_bounds <- result_alternative_DEoptim$optim$bestmem - z_value * standard_errors
upper_bounds <- result_alternative_DEoptim$optim$bestmem + z_value * standard_errors
```
Interpret the Results: The lower_bounds and upper_bounds vectors now contain the lower and upper bounds of the confidence intervals for each parameter. If these intervals are narrow, it indicates that you can be fairly confident about the estimated parameter values. If they are wide, it suggests that there is more uncertainty.
