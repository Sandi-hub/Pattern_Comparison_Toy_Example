---
title: "Comparison Numerical Optimization Methods"
author: "Sandra Rudeloff"
date: "2023-08-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Derivation of the likelihood function

Likelihood function:
$$\mathcal{L}( \theta, \rho) = - \sum_{i=1}^p \mu_i + \sum_{i=1}^p y_i \log{\mu_i}$$

-   $\theta$: risk parameters, in my case $\alpha$ and $\beta$
-   $p$: number of subregions - in my case 100
-   $y_i$: number of cases in each subregion
-   $\mu_i$ expected number of cases in subregion $i$

Expected Number of cases: $$\mu_i = \rho N_i \prod_{l=1}^{k} f_l(x_i)$$

-   $\rho$: scale parameter
-   $N_i$: number of people at risk in subregion $i$, in my case the
    population
-   $f_l(x_i)$: risk function associated with the $l^{th}$ source,
    evaluated at the centroid $x_i$ of subregion $i$.

risk function for distance $d$ from the source:
$$f(d) = 1 + \alpha \exp[- (\frac{d}{\beta})^2]$$

-   $\alpha$: proportional elevation in risk at the source
-   $\beta$: decrease in risk

Given this information, you can use the likelihood function to estimate
the parameters $\alpha, \beta$ and $\rho$ for the model without
covariates.

# Libraries

```{r}
library(readxl)# Read Scenario Definition from Excel
library(dplyr)

# Visualization
library(ggplot2)
library(ggnewscale)
library(RColorBrewer)
library(plotly)

library(reticulate) # connection to python scripts
library(DEoptim) # optimizer

source("generate_population.R")
source("generate_supermarkets.R")
source("visualize_scenario.R")
source("raised_incidence_model_1997.R")
source_python("../Diffusion Model/gravity_model.py")
source_python("../Diffusion Model/outbreak_generation.py")
```

# Input Data
```{r}
investigation_scenario <- 1
no_of_cells <- 100
delta <- 0.05  # Define half the side length of a square
set.seed(333)
```

# Collect Variables
All values are measured in km.

## Population Data

number of people at risk in each subregion $N$
```{r}
population_data <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Population"),scenario_id == investigation_scenario) 
```

```{r}
df_population <- generate_population(population_data, no_of_cells) 
df_population$cell_id <- as.numeric(row.names(df_population)) # assign cell_ids
```


## Shops Data
```{r}
chain_details <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Chain_Details"), scenario_id == investigation_scenario)
```

```{r}
df_shops <- data.frame()

# Loop through each chain in the specific scenario to create the final dataframe
for (current_chain_id in unique(chain_details$chain_id)) {
  chain_data <- chain_details %>% 
    filter(chain_id == current_chain_id)
  
  df_shops_current <- generate_shops(no_of_cells, chain_data)
  df_shops_current$chain <- current_chain_id
  
  df_shops <- rbind(df_shops, df_shops_current)
}
```

Assign the correct cell_id to the stores
```{r}
df_shops <- df_shops %>%
  rowwise() %>%
  mutate(
    cell_id = which(
      (df_population$x_centroid - 0.05) <= store_x &
      (df_population$x_centroid + 0.05) >= store_x &
      (df_population$y_centroid - 0.05) <= store_y &
      (df_population$y_centroid + 0.05) >= store_y
    )
  ) %>%
  ungroup()
```

## Outbreak Data
Convert the df_shops data frame to a Python data frame
```{r}
df_shops_py <- r_to_py(df_shops)
df_population_py <- r_to_py(df_population)
```

```{r}
outbreak_data <- subset(read_excel("./Data/scenarios.xlsx", sheet = "Outbreak"), scenario_id == investigation_scenario)
```

### Calculate Flow of Goods
```{r}
empirical_mean_shopping_distance <- outbreak_data$empirical_mean_shopping_distance
tolerance <- outbreak_data$tolerance
```

```{r}
flow <- hyman_model(empirical_mean_shopping_distance, tolerance,df_population_py, df_shops_py)
```

### Generate Outbreaks for Scenario
```{r}
if (is.character(outbreak_data$outbreak_scenario_sizes)) {
  list_outbreak_scenario_sizes <- as.integer( unlist(strsplit(outbreak_data$outbreak_scenario_sizes, ",")))
} else {
  list_outbreak_scenario_sizes <- as.integer(outbreak_data$outbreak_scenario_sizes)
}

no_of_trials_per_scenario = as.integer(outbreak_data$no_of_trials_per_scenario) #Hier könnte man auch noch eine Logik hinterlegen
```


```{r}
unique_chains <- unique(df_shops$chain)
```

```{r}
# Create a list to store the outbreak data
outbreak_list <- list()

# Loop over each chain, outbreak scenario size, and trial number
for (chain in unique_chains) {  for (no_of_outbreak_cases in list_outbreak_scenario_sizes) {
    for (trial in seq_len(no_of_trials_per_scenario)) {
      
      # Generate the outbreak data
      outbreak <- generate_outbreak(chain, no_of_outbreak_cases, flow, df_shops_py, df_population_py)
      
      # Create a unique identifier for this outbreak scenario
      identifier <- paste(chain, no_of_outbreak_cases, trial, sep = "_")
      
      # Save the outbreak data to the list using the identifier as the name
      outbreak_list[[identifier]] <- outbreak
    }
  }
}
```


```{r}
df_outbreak <- outbreak_list$`Chain 1_10_1`
```


## Visualization / Plotting

```{r}
visualize_scenario(investigation_scenario, df_shops, df_population, df_outbreak)
```


# Model

## Construct Y

Observed number of cases in each subregion

```{r}
y <- get_y(df_population, df_outbreak, delta)
```

## Construct N

Number of people at risk in each subregion

```{r}
N <- get_N(df_population)
```


## Optimize
The max. distance that can occur in the toy example is 
> compute_distance(0.05, 0.05, 0.95, 0.95)
[1] 1.272792

so any value for beta over 50 doesn't make any sense 
```{r}
lower_bounds <- c(alpha = 0.001, beta = 0.0001)
upper_bounds <- c(alpha = 5000, beta = 50) 
```

```{r}
logLik_null <- -likelihood_function_minimize(c(0,0), y=y, N=N, df_population = df_population, df_shops =df_shops)
```

### Deoptim
#### Alternative Model

```{r}
result_alternative_DEoptim <- DEoptim(fn = likelihood_function_minimize, lower = lower_bounds, upper = upper_bounds, y = y, N = N, df_population = df_population, df_shops = df_shops)
print(result_alternative_DEoptim$optim$bestmem)
```

#### Compare the nested Models
```{r}
logLik_alternative_DEoptim <- -result_alternative_DEoptim$optim$bestval
```

```{r}
GLRT_statistic <- 2 * (logLik_alternative_DEoptim - logLik_null) #y! kürzt sich raus

# Determine the degrees of freedom (difference in number of parameters between the two models)
df <- 2  # alpha and beta are the additional parameters in the alternative model

# Compute the p-value
p_value <- 1 - pchisq(GLRT_statistic, df)

# Print the results
print(paste("GLRT statistic:", GLRT_statistic))
print(paste("Degrees of freedom:", df))
print(paste("P-value:", p_value))

# Decide on the hypothesis based on a significance level (e.g., 0.05)
if (p_value < 0.05) {
  cat("Reject the null hypothesis in favor of the alternative.\n")
} else {
  cat("Fail to reject the null hypothesis.\n")
}
```


# Summary
## DeOptim
```{r}
print(paste("alpha: ", result_alternative_DEoptim$optim$bestmem[1], " beta: ", result_alternative_DEoptim$optim$bestmem[2]))
print(paste("likelihood value: ", likelihood_function_minimize(c(result_alternative_DEoptim$optim$bestmem[1], result_alternative_DEoptim$optim$bestmem[2]), y=y, N=N, df_population = df_population, df_shops = df_shops)))
```

# Standard error
```{r}
z_value <- 1.96 # For a 95% confidence level
```

## Hessian 
```{r}
std_error_Hessian <- calculate_std_errors_Hessian(result_alternative_DEoptim, y = y, N=N, df_population, df_shops)
```
standard error gives an idea of how much the estimated parameter is expected to vary from sample to sample.

In your specific case, you're dealing with a likelihood function for which you've estimated parameters (alpha and beta). The Hessian matrix at the maximum likelihood estimates gives you an idea of how "curved" the likelihood function is at that point. The inverse of this Hessian matrix is related to the variances (and covariances) of your parameter estimates. Taking the square root of the diagonal elements gives you the standard errors, which tell you how much those estimates are likely to vary if you were to collect new data and re-estimate the parameters.

```{r}
lower_bounds <- result_alternative_DEoptim$optim$bestmem - z_value * std_error_Hessian
upper_bounds <- result_alternative_DEoptim$optim$bestmem + z_value * std_error_Hessian
```

## Monte Carlo
mplementing Monte Carlo simulations to estimate standard errors involves generating multiple simulated datasets based on your fitted model, then refitting the model to each of these datasets to obtain a distribution of parameter estimates. The standard deviation of this distribution will give you the standard error for each parameter.
```{r}
# Number of simulations
n_simulations <- 10
```


```{r}
std_error_MC <-  calculate_std_errors_MC(result_alternative_DEoptim, n_simulations, df_population, df_shops)
```

# Confidence Intervals
Calculate Confidence Intervals: Use the formula for a confidence interval, which is:

Confidence Interval = Parameter Estimate ± (Z-value × Standard Error)

```{r}
z_value <- 1.96 # For a 95% confidence level
lower_bounds <- result_alternative_DEoptim$optim$bestmem - z_value * std_error_MC
upper_bounds <- result_alternative_DEoptim$optim$bestmem + z_value * std_error_MC
```
Interpret the Results: The lower_bounds and upper_bounds vectors now contain the lower and upper bounds of the confidence intervals for each parameter. If these intervals are narrow, it indicates that you can be fairly confident about the estimated parameter values. If they are wide, it suggests that there is more uncertainty.
